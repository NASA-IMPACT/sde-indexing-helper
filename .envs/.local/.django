# General
# ------------------------------------------------------------------------------
USE_DOCKER=yes
IPYTHONDIR=/app/.ipython
# Redis
# ------------------------------------------------------------------------------
REDIS_URL=redis://redis:6379/0

# Celery
# ------------------------------------------------------------------------------

# Flower
CELERY_FLOWER_USER=eIiBXIcbOYpPoMmLMUcZGaRRxrzuGnSQ
CELERY_FLOWER_PASSWORD=ihNdMn6w9HoofxZ9BVEcxEXvzaxPZf9ByX0xZzQmTnZpYIrtQo3AlzyUEgh9UfHY

DJANGO_ACCOUNT_ALLOW_REGISTRATION=False

# AWS
# needed for loading scraped urls from s3
# ------------------------------------------------------------------------------
DJANGO_AWS_ACCESS_KEY_ID=''
DJANGO_AWS_SECRET_ACCESS_KEY=''
DJANGO_AWS_STORAGE_BUCKET_NAME=''

# GitHub (please create a new file called .env and put these in there)
# ------------------------------------------------------------------------------
GITHUB_ACCESS_TOKEN=''
SINEQUA_CONFIGS_GITHUB_REPO=''
GITHUB_BRANCH_FOR_WEBAPP=''

#Airflow
AIRFLOW_USERNAME = ''
AIRFLOW_PASSWORD = ''
AIRFLOW_DAG_ENDPOINT = 'http://airflow-webserver-1185199286.us-west-2.elb.amazonaws.com/api/v1/dags/chunck_urls_to_process/dagRuns'

